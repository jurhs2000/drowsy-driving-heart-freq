No path specified. Models will be saved in: "AutogluonModels\ag-20231019_065718\"
Warning: Training may take a very long time because `time_limit` was not specified and `train_data` is large (6514296 samples, 416.92 MB).
        Consider setting `time_limit` to ensure training finishes within an expected duration or experiment with a small portion of `train_data` to identify an ideal `presets` and `hyperparameters` configuration.
Beginning AutoGluon training ...
AutoGluon will save models to "AutogluonModels\ag-20231019_065718\"
AutoGluon Version:  0.8.2
Python Version:     3.9.12
Operating System:   Windows
Platform Machine:   AMD64
Platform Version:   10.0.22621
Disk Space Avail:   635.73 GB / 1024.19 GB (62.1%)
Train Data Rows:    6514296
Train Data Columns: 7
Label Column: level
Preprocessing data ...
AutoGluon infers your prediction problem is: 'multiclass' (because dtype of label-column == int, but few unique label-values observed).
        3 unique label values:  [0, 1, 2]
        If 'multiclass' is not the correct problem_type, please manually specify the problem_type parameter during predictor init (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])
Train Data Class Count: 3
Using Feature Generators to preprocess the data ...
Fitting AutoMLPipelineFeatureGenerator...
        Available Memory:                    3884.8 MB
        Train Data (Original)  Memory Usage: 364.8 MB (9.4% of available memory)
        Warning: Data size prior to feature transformation consumes 9.4% of available memory. Consider increasing memory or subsampling the data to avoid instability.
        Inferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.
        Stage 1 Generators:
                Fitting AsTypeFeatureGenerator...
                        Note: Converting 2 features to boolean dtype as they only contain 2 unique values.
        Stage 2 Generators:
                Fitting FillNaFeatureGenerator...
        Stage 3 Generators:
                Fitting IdentityFeatureGenerator...
        Stage 4 Generators:
                Fitting DropUniqueFeatureGenerator...
        Stage 5 Generators:
                Fitting DropDuplicatesFeatureGenerator...
        Types of features in original data (raw dtype, special dtypes):
                ('float', []) : 1 | ['dateTime']
                ('int', [])   : 6 | ['participant', 'logId', 'bpm', 'age', 'sex', ...]
        Types of features in processed data (raw dtype, special dtypes):
                ('float', [])     : 1 | ['dateTime']
                ('int', [])       : 4 | ['participant', 'logId', 'bpm', 'age']
                ('int', ['bool']) : 2 | ['sex', 'person']
        8.5s = Fit runtime
        7 features in original data used to generate 7 features in processed data.
        Train Data (Processed) Memory Usage: 273.6 MB (7.0% of available memory)
Data preprocessing and feature engineering runtime = 9.72s ...
AutoGluon will gauge predictive performance using evaluation metric: 'accuracy'
        To change this, specify the eval_metric parameter of Predictor()
Automatically generating train/validation split with holdout_frac=0.01, Train Rows: 6449153, Val Rows: 65143
User-specified model hyperparameters to be fit:
{
        'NN_TORCH': {},
        'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],
        'CAT': {},
        'XGB': {},
        'FASTAI': {},
        'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],
        'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],
        'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],
}
Fitting 13 L1 models ...
Fitting model: KNeighborsUnif ...
        Warning: Not enough memory to safely train model. Estimated to require 0.650 GB out of 3.544 GB available memory (91.706%)... (20.000% of avail memory is the max safe size)
        To force training the model, specify the model hyperparameter "ag.max_memory_usage_ratio" to a larger value (currently 1.0, set to >=0.97 to avoid the error)
                To set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={"ag.max_memory_usage_ratio": VALUE})`
        Not enough memory to train KNeighborsUnif... Skipping this model.
Fitting model: KNeighborsDist ...
        Warning: Not enough memory to safely train model. Estimated to require 0.650 GB out of 3.535 GB available memory (91.961%)... (20.000% of avail memory is the max safe size)
        To force training the model, specify the model hyperparameter "ag.max_memory_usage_ratio" to a larger value (currently 1.0, set to >=0.97 to avoid the error)
                To set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={"ag.max_memory_usage_ratio": VALUE})`
        Not enough memory to train KNeighborsDist... Skipping this model.
Fitting model: NeuralNetFastAI ...
        Warning: Not enough memory to safely train model. Estimated to require 3.225 GB out of 3.533 GB available memory (101.407%)... (90.000% of avail memory is the max safe size)
        To force training the model, specify the model hyperparameter "ag.max_memory_usage_ratio" to a larger value (currently 1.0, set to >=1.06 to avoid the error)
                To set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={"ag.max_memory_usage_ratio": VALUE})`
                Setting "ag.max_memory_usage_ratio" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.
        Not enough memory to train NeuralNetFastAI... Skipping this model.
Fitting model: LightGBMXT ...
        Warning: Potentially not enough memory to safely train model. Estimated to require 2.499 GB out of 3.514 GB available memory (79.020%)... (90.000% of avail memory is the max safe size)
        To avoid this warning, specify the model hyperparameter "ag.max_memory_usage_ratio" to a larger value (currently 1.0, set to >=1.00 to avoid the warning)
                To set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={"ag.max_memory_usage_ratio": VALUE})`
[1000]  valid_set's multi_error: 0.4089
[2000]  valid_set's multi_error: 0.407534
[3000]  valid_set's multi_error: 0.406337
[4000]  valid_set's multi_error: 0.405047
[5000]  valid_set's multi_error: 0.403988
[6000]  valid_set's multi_error: 0.403251
[7000]  valid_set's multi_error: 0.402039
[8000]  valid_set's multi_error: 0.400764
[9000]  valid_set's multi_error: 0.400074
[10000] valid_set's multi_error: 0.399429
        0.6006   = Validation score   (accuracy)
        4252.75s         = Training   runtime
        41.52s   = Validation runtime
Fitting model: LightGBM ...
        Warning: Not enough memory to safely train model. Estimated to require 2.499 GB out of 1.938 GB available memory (143.312%)... (90.000% of avail memory is the max safe size)
        To force training the model, specify the model hyperparameter "ag.max_memory_usage_ratio" to a larger value (currently 1.0, set to >=1.48 to avoid the error)
                To set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={"ag.max_memory_usage_ratio": VALUE})`
                Setting "ag.max_memory_usage_ratio" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.
        Not enough memory to train LightGBM... Skipping this model.
Fitting model: RandomForestGini ...
        Warning: Not enough memory to safely train model. Estimated to require 12.898 GB out of 1.926 GB available memory (1339.533%)... (50.000% of avail memory is the max safe size)
        To force training the model, specify the model hyperparameter "ag.max_memory_usage_ratio" to a larger value (currently 1.0, set to >=13.45 to avoid the error)
                To set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={"ag.max_memory_usage_ratio": VALUE})`
                Setting "ag.max_memory_usage_ratio" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.
        Not enough memory to train RandomForestGini... Skipping this model.
Fitting model: RandomForestEntr ...
        Warning: Not enough memory to safely train model. Estimated to require 12.898 GB out of 1.920 GB available memory (1343.854%)... (50.000% of avail memory is the max safe size)
        To force training the model, specify the model hyperparameter "ag.max_memory_usage_ratio" to a larger value (currently 1.0, set to >=13.49 to avoid the error)
                To set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={"ag.max_memory_usage_ratio": VALUE})`
                Setting "ag.max_memory_usage_ratio" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.
        Not enough memory to train RandomForestEntr... Skipping this model.
Fitting model: CatBoost ...
        Warning: Not enough memory to safely train model. Estimated to require 2.499 GB out of 1.930 GB available memory (129.507%)... (100.000% of avail memory is the max safe size)
        To force training the model, specify the model hyperparameter "ag.max_memory_usage_ratio" to a larger value (currently 1.0, set to >=1.35 to avoid the error)
                To set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={"ag.max_memory_usage_ratio": VALUE})`
                Setting "ag.max_memory_usage_ratio" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.
        Not enough memory to train CatBoost... Skipping this model.
Fitting model: ExtraTreesGini ...
        Warning: Not enough memory to safely train model. Estimated to require 12.898 GB out of 1.930 GB available memory (1336.735%)... (50.000% of avail memory is the max safe size)
        To force training the model, specify the model hyperparameter "ag.max_memory_usage_ratio" to a larger value (currently 1.0, set to >=13.42 to avoid the error)
                To set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={"ag.max_memory_usage_ratio": VALUE})`
                Setting "ag.max_memory_usage_ratio" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.
        Not enough memory to train ExtraTreesGini... Skipping this model.
Fitting model: ExtraTreesEntr ...
        Warning: Not enough memory to safely train model. Estimated to require 12.898 GB out of 1.966 GB available memory (1312.223%)... (50.000% of avail memory is the max safe size)
        To force training the model, specify the model hyperparameter "ag.max_memory_usage_ratio" to a larger value (currently 1.0, set to >=13.17 to avoid the error)
                To set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={"ag.max_memory_usage_ratio": VALUE})`
                Setting "ag.max_memory_usage_ratio" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.
        Not enough memory to train ExtraTreesEntr... Skipping this model.
Fitting model: XGBoost ...
        Warning: Not enough memory to safely train model. Estimated to require 2.499 GB out of 1.949 GB available memory (128.236%)... (100.000% of avail memory is the max safe size)
        To force training the model, specify the model hyperparameter "ag.max_memory_usage_ratio" to a larger value (currently 1.0, set to >=1.33 to avoid the error)
                To set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={"ag.max_memory_usage_ratio": VALUE})`
                Setting "ag.max_memory_usage_ratio" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.
        Not enough memory to train XGBoost... Skipping this model.
Fitting model: NeuralNetTorch ...
        Warning: Potentially not enough memory to safely train model. Estimated to require 1.290 GB out of 1.926 GB available memory (74.400%)... (90.000% of avail memory is the max safe size)
        To avoid this warning, specify the model hyperparameter "ag.max_memory_usage_ratio" to a larger value (currently 1.0, set to >=0.94 to avoid the warning)
                To set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={"ag.max_memory_usage_ratio": VALUE})`   
        0.5951   = Validation score   (accuracy)
        12369.63s        = Training   runtime
        0.24s    = Validation runtime
Fitting model: LightGBMLarge ...
        Warning: Not enough memory to safely train model. Estimated to require 2.499 GB out of 2.437 GB available memory (113.957%)... (90.000% of avail memory is the max safe size)
        To force training the model, specify the model hyperparameter "ag.max_memory_usage_ratio" to a larger value (currently 1.0, set to >=1.19 to avoid the error)       
                To set the same value for all models, do the following when calling predictor.fit: `predictor.fit(..., ag_args_fit={"ag.max_memory_usage_ratio": VALUE})`   
                Setting "ag.max_memory_usage_ratio" to values above 1 may result in out-of-memory errors. You may consider using a machine with more memory as a safer alternative.
        Not enough memory to train LightGBMLarge... Skipping this model.
Fitting model: WeightedEnsemble_L2 ...
        0.6007   = Validation score   (accuracy)
        1.53s    = Training   runtime
        0.01s    = Validation runtime
AutoGluon training complete, total runtime = 16696.38s ... Best model: "WeightedEnsemble_L2"
TabularPredictor saved. To load, use: predictor = TabularPredictor.load("AutogluonModels\ag-20231019_065718\")
                 model  score_test  score_val  pred_time_test  pred_time_val  ...  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order
0           LightGBMXT    0.601454   0.600571     4808.122086      41.520334  ...               41.520334        4252.753939            1       True          1
1  WeightedEnsemble_L2    0.601110   0.600663     4826.654127      41.765424  ...                0.005940           1.531833            2       True          3
2       NeuralNetTorch    0.594184   0.595075       18.064998       0.239151  ...                0.239151       12369.630184            1       True          2

[3 rows x 12 columns]
Computing feature importance via permutation shuffling for 7 features using 5000 rows with 5 shuffle sets...
        183.86s = Expected runtime (36.77s per shuffle set)
        145.12s = Actual runtime (Completed 5 of 5 shuffle sets)